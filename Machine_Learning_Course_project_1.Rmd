---
title: 'Machine Learning Course Project: Predicting the manner of doing excercise
  based on parametric accelerometers data'
author: "Andrey"
date: "23 апрел€ 2016 г."
output: html_document
---
# Background 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement Ц a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

# Source Data
The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

# Purpose of the project / outcome 
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

# Loading the data

```{r, cache=TRUE}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
set.seed(12345)

```
Whebn we first take a look at the data (e.g. downloading from the link above and oping in XL spreadsheet) we observe that

    The header contains the column names 
    There are many missing values, which can be split into 3 types: NA, "","#DIV/0!"
    The first column is not a variable, it counts the row number.
    
    
```{r, cache=TRUE}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```

Checking dim of 2 datasets, we conclude that both have 160 variables and different number of observatrions
```{r, cache=TRUE}
dim(training); dim(testing)
```

Looking at the variable CLASSE which we need to predict, we conclude that this is a "factor"-class variable which can take one of 5 meanings: 

```{r, cache=TRUE, echo=FALSE}
class(training$classe)
q <- as.data.frame (table(training$classe))
q
```

# CROSS-VALIDATION strategy and model selection strategy

Cross-validation STRATEGY: We'll split available Training dataset into 2 subsets: training1 (65%) and testing1 (35%). 

We'll use 2 methods (Desision Trees and Random Forests) to train them on training1 sample and predict on testing 1 sample

We'll compare the models for accuracy/ out of sample error (received by predicting on testing1) and we'll choose the best model based out of sample error 

We'll use te best model to predict on the original (testing) dataset which has 20 observations in it to predict the CLASSE variable

```{r, cache=TRUE}
train_partition <- createDataPartition(y=training$classe, p=0.65, list=FALSE)
training1 <- training[train_partition, ]
testing1 <- training[-train_partition, ]
dim(training1); dim(testing1)
```

We want to identify and remove all variables which have NearZeroVariance 

```{r, cache=TRUE}
nzv <- nearZeroVar(training1, saveMetrics=TRUE)
training1 <- training1[,nzv$nzv==FALSE]
nzv<- nearZeroVar(testing1,saveMetrics=TRUE)
testing1 <- testing1[,nzv$nzv==FALSE]
```

We want to remove the first column, which is simply the order of record but not a variable

```{r, cache=TRUE}
training1 <- training1[c(-1)]
```

Now we identify vatiables which have mostly NA. Those columns where NA are more than 60% will be removed  
```{r, cache=TRUE}
training1_temp <- training1 
for(i in 1:length(training1)) { 
        if( sum( is.na( training1[, i] ) ) /nrow(training1) >= .6 ) { 
        for(j in 1:length(training1_temp)) {
            if( length( grep(names(training1[i]), names(training1_temp)[j]) ) ==1)  { 
                training1_temp <- training1_temp[ , -j] 
            }   
        } 
    }
}
```

After checking how many columns / variables left after data cleaning, we conclude that their number reduced from 160 to 58

```{r, cache=TRUE}
dim(training1_temp)
```

resuming to our training subset
```{r, cache=TRUE}
training1 <- training1_temp
rm(training1_temp)
```
 We need to repaet the  same data cleaining for testing1 subset and for original testing dataset  

```{r, cache=TRUE}
clean1 <- colnames(training1)
clean2 <- colnames(training1[, -58]) 
testing1 <- testing1[clean1]
testing <- testing[clean2]

```

Making sure the number of columns has reduced acordingly 

```{r, cache=TRUE}
dim(testing1); dim(testing)

```
We want to coerce the data into the same type

```{r, cache=TRUE}
for (i in 1:length(testing) ) {
        for(j in 1:length(training1)) {
        if( length( grep(names(training1[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(training1[i])
        }      
    }      
}

testing <- rbind(training1[2, -58] , testing) 
testing <- testing[-1,]
```

# Predicion using Decision Trees 

```{r, cache=TRUE}
modFitA1 <- rpart(classe ~ ., data=training1, method="class")

```

```{r, cache=TRUE}
fancyRpartPlot(modFitA1)
```

Predicting and checking accuracy using confusion matrix 

```{r, cache=TRUE}
predictionsA1 <- predict(modFitA1, testing1, type = "class")
confmatrix_tree <- confusionMatrix(predictionsA1, testing1$classe)
confmatrix_tree
plot(confmatrix_tree$table, col = confmatrix_tree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(confmatrix_tree$overall['Accuracy'], 4)))
```

Accuracy for the Decision trees methid for dataset testing1 is 0.879

The expected OUT OF SAMPLE ERROR (for testing1 dataset) for the Decision Trees method is 100-87.9 = 12.1% 

# Predictions using Random Forests

```{r, cache=TRUE}
modFitB1 <- randomForest(classe ~. , data=training1)
predictionsB1 <- predict(modFitB1, testing1, type = "class")
confmatrix_randfor <- confusionMatrix(predictionsB1, testing1$classe)
confmatrix_randfor
plot(modFitB1)
plot(confmatrix_randfor$table, col = confmatrix_tree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(confmatrix_randfor$overall['Accuracy'], 4)))
```
Accuracy for the Random forests methid for dataset testing1 is 0.879

The expected OUT OF SAMPLE ERROR (for testing1 dataset) for the random forests method is 100-99.87 = 0.13% 

# Predicting CLASSE variable for the data in the Test dataset :

Random Forests gave a better accuracy on the testing1 dataset of 99.94% (compared to 86.6% for Decision Trees),  hence OUT OF SAMPLE ERROR is smaller for Random Forests method (0.13% vs 12.1%) 

so we'll select Random Forests method to predict CLASSE variable for the Test sample.

```{r, cache=TRUE}
predictionsB2 <- predict(modFitB1, testing, type = "class")
predictionsB2 
```


